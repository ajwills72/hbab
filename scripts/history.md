# Lecture 1: History

## Goals for today

The goals for today's two-hour session are to help you understand the history, and core concepts, of brain-inspired artificial intelligence, starting from 1943 as we began to realise the brain was an electrical network of neurons, through to 1986, when "connectionism" became mainstream in psychology. 

The core concepts are:


Activations (firing rates)

Weights (synaptic plasticity)

Spreading activation

Learning rule (Delta rule)

Perceptron convergence theorem - AI optimism. 

XOR - AI pessimism

Backprop - (re)birth of connectionism

How brain-like is backprop?


## Demonstrations in keras

- [Minimal XOR solving with backprop](https://github.com/conwayok/keras-xor-example/blob/master/keras-xor-example/train.py)


## History

- [Perceptrons, the book (wikipedia)](https://en.wikipedia.org/wiki/Perceptrons_(book))

- [Blog on perceptrons, including a nice Rosenblatt and machine pic](https://fiascodata.blogspot.com/2018/05/a-computer-program-is-said-tolearn-from.html)

